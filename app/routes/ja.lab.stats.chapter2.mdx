---
title: Principal Component Analysis (PCA)
---

import '../lab.css'
import { KaTeX } from '../components/utils/Katex'

## 2.1 主成分分析（PCA）の理論的背景

主成分分析（Principal Component Analysis, PCA）は、高次元のデータを低次元に圧縮するための手法であり、線形の次元削減法として最も広く使用されています。PCAの基本的な目標は、データ内の最も重要な情報（分散）が最大化されるように、新しい軸を定義することです。これにより、次元数を削減してもデータの本質的な特徴を保持することができます。PCAの理論的背景は、以下のステップで構成されています。

### 共分散行列とその計算

PCAを理解するための第一歩は、**共分散行列**の計算です。共分散行列は、データセット内の特徴量間の線形関係を示す行列であり、PCAの核心を成す要素となります。まず、$X$というデータ行列があるとしましょう。各列は異なる特徴量（例えば、身長、体重、年齢など）を表し、各行はサンプル（個別のデータポイント）を表します。

共分散行列は次のように計算されます。

<KaTeX expression="C = \frac{1}{n-1} X^T X" isFormula={true} />

ここで、$X^T$はデータ行列$X$の転置行列、$C$は共分散行列、$n$はデータポイントの数です。共分散行列の各要素$C_{ij}$は、特徴量$i$と特徴量$j$の共分散を表します。共分散が正の値の場合、2つの特徴量は同じ方向に変動し、負の値の場合は逆方向に変動することを意味します。共分散がゼロの場合、2つの特徴量は独立しており、線形関係はないと言えます。

### 例題：2次元データの共分散行列

例えば、以下のような2次元のデータがあるとします。

| サンプル | 特徴量1（身長） | 特徴量2（体重） |
| -------: | --------------: | --------------: |
|        1 |             160 |              55 |
|        2 |             170 |              70 |
|        3 |             180 |              80 |
|        4 |             165 |              60 |
|        5 |             175 |              75 |

まず、各特徴量の平均を計算します。

- 身長の平均: $\mu_1 = \frac{160 + 170 + 180 + 165 + 175}{5} = 170$
- 体重の平均: $\mu_2 = \frac{55 + 70 + 80 + 60 + 75}{5} = 68$

次に、データ行列から平均を引き、共分散行列を計算します。データを中心化した後、以下の共分散行列を計算します。

<KaTeX
	expression="C = \frac{1}{4} \begin{pmatrix} 25 & 50 \\ 50 & 100 \end{pmatrix}"
	isFormula={true}
/>

この共分散行列から、身長と体重がどの程度関連しているか、またそれぞれの特徴量がどのように分散しているかがわかります。具体的には、共分散行列の$C_{12}$の値（50）は、身長と体重が正の線形関係にあることを示しており、身長が高くなると体重も増える傾向があることがわかります。

### 固有値問題と主成分の抽出

PCAは、共分散行列の固有値問題を解くことによって、データの主成分（新しい基底）を抽出します。共分散行列$C$の固有値と固有ベクトルを求めることで、データの分散を最大化する方向を特定します。固有値は、各主成分に沿った分散の大きさを示し、固有ベクトルはその方向を示します。

- 固有値の大きい順に、主成分（固有ベクトル）を選びます。
- 最も大きな固有値に対応する固有ベクトルが、最初の主成分となります。
- 次に大きな固有値に対応する固有ベクトルが、2番目の主成分となります。

新しいデータは、これらの固有ベクトルに基づいて変換され、次元削減が行われます。例えば、2次元データであれば、2つの主成分を選ぶことができます。

### 主成分の数の選定

PCAを実行する際に重要なのは、**主成分の数**をどう選ぶかです。主成分の数は、通常、累積分散比率（Explained Variance Ratio）に基づいて決定されます。この比率は、各主成分がどれだけ元のデータの分散を説明できるかを示します。例えば、最初の2つの主成分で80%以上の分散を説明できる場合、これら2つの主成分を選択することが適切です。

## 2.2 主成分分析の応用

### 次元削減とデータ圧縮

PCAの最も一般的な応用は、次元削減です。データの次元を削減することで、計算資源を節約し、視覚的にデータを理解しやすくすることができます。例えば、顔認識において、元々は数百次元に及ぶ画像データを、PCAによって数十次元に圧縮し、計算量を大幅に削減できます。

### 特徴量選択とノイズ除去

PCAはまた、データのノイズを減らし、重要な特徴量を抽出するためにも使用されます。PCAによって、最も分散が大きい方向に基づいて特徴量が選ばれるため、データセットの中で本質的な特徴を強調することができます。これにより、ノイズが除去され、モデルの性能が向上します。

## 2.3 PCAの限界と高度な手法

### PCAの限界

PCAは線形の手法であり、データの非線形構造を捉えることができません。このため、データが非線形の関係を持っている場合、PCAだけではその関係を捉えることができません。また、PCAは分散を最大化する方向を選ぶため、必ずしもデータの最も重要な特徴を選ぶわけではありません。

### 非線形次元削減手法（t-SNE, UMAP）

PCAの限界を補うために、t-SNEやUMAPといった非線形次元削減手法が開発されました。これらの手法は、データの非線形な構造を捉えることができ、特にデータが複雑な場合に有効です。

### カーネルPCA

カーネルPCAは、非線形データを扱うためのPCAの拡張です。カーネル法を使用してデータを高次元空間にマッピングし、その空間でPCAを実行することで、非線形なデータ構造を捉えることができます。

## まとめ

PCAは、次元削減、特徴量選択、ノイズ除去など、多くの分野で非常に有効な手法です。しかし、非線形のデータには限界があり、その場合はt-SNEやUMAP、カーネルPCAといった他の手法を使用することが必要です。
