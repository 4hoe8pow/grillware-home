---
title: Principal Component Analysis (PCA): Theory and Applications
---

import '../lab.css'
import { KaTeX } from '../components/utils/Katex'

## 2.1 主成分分析（PCA）の理論的背景

主成分分析（Principal Component Analysis, PCA）は、高次元のデータを低次元に圧縮するための強力な線形手法であり、機械学習や統計解析、データサイエンスの分野で最も広く利用されています。PCAの主要な目標は、データの分散を最大化する軸を新たに定義することにより、データの本質的な特徴を維持しつつ次元を削減することです。この手法は、データの解析や視覚化、さらには特徴量選択において非常に有効です。PCAの理論的背景は以下のステップに分けて理解できます。

### 共分散行列とその計算

PCAを理解するための第一歩は、**共分散行列**の計算です。共分散行列は、データセット内の異なる特徴量間の線形関係を示す行列であり、PCAの核心を成す要素です。まず、$X$というデータ行列があるとしましょう。この行列の各列は異なる特徴量（例えば、身長、体重、年齢など）を表し、各行はサンプル（個別のデータポイント）を表します。

共分散行列の計算は次の式で表されます。

<KaTeX expression="C = \frac{1}{n-1} X^T X" isFormula={true} />

ここで、$X^T$はデータ行列$X$の転置行列、$C$は共分散行列、$n$はデータポイントの数を示します。共分散行列の各要素$C_{ij}$は、特徴量$i$と特徴量$j$の共分散を表します。共分散が正の値の場合、2つの特徴量は同じ方向に変動し、負の値の場合は逆方向に変動することを意味します。共分散がゼロの場合、2つの特徴量は独立しており、線形関係がないことを示します。

### 例題：2次元データの共分散行列

以下のような2次元のデータセットがあると仮定しましょう。

| サンプル | 特徴量1（身長） | 特徴量2（体重） |
| -------: | --------------: | --------------: |
|        1 |             160 |              55 |
|        2 |             170 |              70 |
|        3 |             180 |              80 |
|        4 |             165 |              60 |
|        5 |             175 |              75 |

まず、各特徴量の平均を計算します。

-   身長の平均: $\mu_1 = \frac{160 + 170 + 180 + 165 + 175}{5} = 170$
-   体重の平均: $\mu_2 = \frac{55 + 70 + 80 + 60 + 75}{5} = 68$

次に、データ行列から平均を引き、中心化を行った後、共分散行列を計算します。中心化されたデータは、各特徴量が平均値を持つように調整されたデータです。これにより、分散を正確に捉えることが可能になります。

<KaTeX
	expression="C = \frac{1}{4} \begin{pmatrix} 25 & 50 \\ 50 & 100 \end{pmatrix}"
	isFormula={true}
/>

この共分散行列から、身長と体重の相関関係を視覚的に確認できます。共分散行列の$C_{12}$の値（50）は、身長と体重が正の線形関係にあることを示しており、身長が高くなると体重も増加する傾向が見て取れます。

### 固有値問題と主成分の抽出

PCAの中心的な操作は、共分散行列の**固有値**と**固有ベクトル**を求めることです。これによって、データの分散を最大化する新しい軸（主成分）を発見します。具体的には、次のプロセスが行われます。

1. 共分散行列$C$の固有値問題を解きます。
2. 各固有値に対応する固有ベクトル（主成分）を抽出します。
3. 最も大きな固有値に対応する固有ベクトルが最初の主成分として選ばれ、次に大きな固有値に対応する固有ベクトルが次の主成分として選ばれます。

固有値は、各主成分に沿った分散の大きさを示し、固有ベクトルはその方向を示します。この操作によって、データの重要な特徴を保持しながら次元削減を行うことができます。

### 主成分の数の選定

PCAを使用する際に重要なのは、削減する次元数（主成分の数）をどのように選定するかです。これには、**累積分散比率**（Explained Variance Ratio）を使用します。これは、各主成分がデータの分散をどれだけ説明するかを示す指標です。例えば、最初の2つの主成分で元のデータの80%以上の分散を説明できる場合、次元削減後もデータの本質的な特徴を十分に保持できていると言えます。

## 2.2 主成分分析の応用

### 次元削減とデータ圧縮

PCAの最も一般的な応用は、**次元削減**です。データの次元を削減することで、計算資源を節約し、データを視覚的に理解しやすくすることができます。たとえば、顔認識において、元々数百次元に及ぶ画像データをPCAで数十次元に圧縮し、計算量を大幅に削減することが可能です[^1]。これにより、処理速度の向上が期待できます。

[^1]: PCAは、顔画像データの圧縮だけでなく、画像分類にも広く使用されています。顔認識においては、「Eigenfaces」と呼ばれる方法が有名です。

### 特徴量選択とノイズ除去

PCAは、**特徴量選択**や**ノイズ除去**にも非常に有効です。次元削減の過程で、分散が小さい方向の成分は除外されるため、データセットの本質的な特徴が強調されます。この特性を活用することで、ノイズを削除し、重要な特徴を抽出することができます。PCAは、データの最も重要な構造を見つけるため、機械学習の前処理としてよく使用されます。

## 2.3 PCAの限界と高度な手法

### PCAの限界

PCAはあくまで**線形**な手法であり、非線形なデータには適用できません。例えば、非線形のデータ構造を持つ問題に対してPCAを使用すると、データの重要な特徴が失われる可能性があります。また、PCAは分散を最大化する軸を選ぶため、必ずしもデータにとって最も重要な特徴を選択するとは限りません。

### 非線形次元削減手法（t-SNE, UMAP）

PCAの限界を補うために、**t-SNE**や**UMAP**などの非線形次元削減手法が開発されました。これらの手法は、データの非線形な関係を捉えることができ、特に複雑なデータセットにおいて優れた性能を発揮します。

### カーネルPCA

非線形データに対処するために、**カーネルPCA**が提案されています。カーネルPCAは、カーネル法を用いてデータを高次元空間にマッピングし、その空間でPCAを適用することで、非線形構造を捉えることが可能になります。カーネルPCAは、特に画像認識や音声認識といった複雑な問題において有効です。

## まとめ

PCAは次元削減、特徴量選択、ノイズ除去など多くの分野で重要な手法です。しかし、非線形データには限界があり、これに対してはt-SNEやUMAP、カーネルPCAなどの高度な手法を使用することが推奨されます。PCAを正しく理解し、適切に使用することで、データ解析の効率と精度を大幅に向上させることができます。
